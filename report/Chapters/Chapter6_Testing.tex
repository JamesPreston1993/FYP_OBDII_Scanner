\section{Introduction}
	\paragraph{}{
	This chapter will outline the processes that were utilised to test and evaluate the system developed as part of this project. This will include the testing of functional and non-functional requirements as well as the rating of the usability of the application and software quality.
	}
	
\section{Support for Functional Requirements}
	%Two types: Bench testing, real world testing, both capture different things
	\paragraph{}{
	Support for the functional requirements was difficult to evaluate given the domain of the system. This is because the system requires the retrieval of data from a vehicle, which cannot be manually manipulated by the user, for example, by setting a specific DTC or data value. While it is still possible to manipulate some data, for example, pressing the accelerator changes the engine speed value, it would require either a large base of vehicles for testing or the design and creation of a specific hardware device that would act as a configurable ECU, both of which were outside the scope of this project.
	%Intro
	}

	\paragraph{}{
	To counteract this, testing was split into two approaches: bench testing and real world testing. Bench testing utilised the simulation communication system, outlined in section \ref{sec:Simulation}, whereas real world testing utilised live vehicles and the ECU development environment to test the functional requirements of the system. 
	%Bench testing
	}
	
	\paragraph{}{
	Bench testing was utilised for scenarios that required direct and controlled manipulation of data. This was mostly for the testing of user interface elements, such as smart graphing in the data module. Bench testing was carried out on each module in the application to ensure that the information presented to the user on-screen matched what was interpreted from the communication system. Table \ref{tab:BenchTest} shows a subset of the bench tests that were performed.
	
	
	%Bench testing
	% - UI
	% - Extreme scenarios that require control of the values
	% - 
	}
	
	\paragraph{}{
	Real world testing involved connecting to vehicles or ECUs to test the communication system of the application. This approach was utilised to test the functionality of low level communication, such as the Bluetooth connection and data conversion. To verify the outputs of the system, they were compared to those of the similar applications discussed previously in this report. The purpose of real world testing was to ensure the functional requirement of being able to connect to and communicate with a live vehicle was met. Table \ref{tab:RealTest} shows a subset of the real world tests that were performed.
	
	
	% Real world testing
	% - Connection status of ELM327
	% - Converting data from the ECU
	% - Clear codes functionality
	}
	
	\begin{table}[ht]
		\begin{center}				
			\begin{tabularx}{\textwidth}{| X | l |}
				\hline
				\textbf{Test Description} & \textbf{Result}\\
				\hline
				Smart-graphing colours match the data value type & Pass\\
				\hline
				Current value is updated when step forward and step back are pressed & Pass\\
				\hline
				DTC Lists are refreshed after Clear Codes is pressed & Pass\\
				\hline				
			\end{tabularx}
			\caption{Bench Testing Tests}
			\label{tab:BenchTest}
		\end{center}
	\end{table}
	
	\begin{table}[ht]
		\begin{center}				
			\begin{tabularx}{\textwidth}{| X | l |}
				\hline
				\textbf{Test Description} & \textbf{Result}\\
				\hline
				Connection module detects all paired devices & Pass\\
				\hline
				The vehicle communication protocol is correctly identified & Pass\\
				\hline
				All supported pids are returned & Pass\\
				\hline
				Engine speed data value increases when accelerator is pressed & Pass\\
				\hline				
				Pressing Clear Codes removes all Current DTCs from the DTC List & Pass\\
				\hline
				Notification is displayed on application shutdown / sleep & Pass\\
				\hline				
			\end{tabularx}
			\caption{Real World Testing Tests}
			\label{tab:RealTest}
		\end{center}
	\end{table}
	
	\paragraph{}{
	Using these two approaches allowed for the testing of each diagnostic mode that was implemented in the system. This included robust testing of the user interface and the communication systems, which served to verify that the functional requirements of the project had been satisfied. 
	}
	
	%Two approaches to functional testing of the system were taken: bench testing and real world testing. Bench testing utilised the simulation communication system to test functionality that did not directly rely on communication with a live vehicle for validation. This helped to test the UI elements as well as any extreme scenarios in the communication systems. Real world testing utilised live vehicles and the ECU development environment to test functionality relating to connection and communication with a vehicle.	
	%Test each module, test each mode, code was always subjected to regression testing before being committed
	%Regression - If bug was small or caused by changes in the commit, it was fixed immediately, if the bug was larger or due to changes outside of the commit, it was deferred to a later date
	
		
\section{Support for Non-functional Requirements}	
	\paragraph{}{
	The support for the non-functional requirements of the system was evaluated through a number of techniques.
	% Intro
	}
	\paragraph{}{
	The extensibility of the system was measured through the code metrics gathered as part of the evaluation of software quality. A more detailed description of each metric can be found in section \ref{sec:Quality}, however, the overall high maintainability index of the system showcased the extensibility of the architecture.
	% Extensibility
	}
	\paragraph{}{
	The system met the requirement of portability, as the end product was demonstrated to run on both Android and Windows 10 using a shared code base. While it is difficult to quantify the overall portability of the system, it can be determined by the fact that, with the exception of moving the shared code out of the existing UWP project, little to no code changes were required in the core library in order to run the application on Android.
	% Portablity
	}
	\paragraph{}{
	The security of the system was evaluated by ensuring that no damage could be done to the vehicle through accidental or intentional misuse of the application. This was difficult as the system could not be tested on live vehicles due to the implications in the case of the application causing damage to the vehicle. Instead, a number of ECUs were used to mitigate risk. Testing included unplugging the ELM327 device during communication and attempting to connect to the same ELM327 device with multiple tablets. After these tests passed, it was deemed that the requirement of security had been met.
	% Security
	}
	\paragraph{}{
	The usability of the system was evaluated through a usability survey involving domain experts, such as mechanics. The results of the survey were then analysed to determine the overall usability of the system. The process for creating and evaluating the results of the survey is discussed in detail in section \ref{sec:Usability}.
	% Usability
	}
	
\section{Usability Testing}{
	\paragraph{}{
	In order to evaluate the usability of the system, it was important to undertake a usability study with potential users. This study was approved by the Faculty of Science and Engineering Ethics Committee, as shown in Appendix A.
	%[INTRO]
	}
	
	\paragraph{}{
	%[HOW SURVEY WAS CREATED]
	As a significant figure in the area of usability, Nielsen's usability heuristics \cite{Heuristics} were utilised when creating the survey. It was ensured that each usability heuristic outlined by Nielsen could be evaluated by a corresponding question on the survey. The result was a quantitative survey, seen in Appendix B, that asked participants to rate each question from strongly disagree to strongly agree. This allowed for results from each question to be directly compared and to retrieve statistical information about the usability of the application. It was also decided to include a section at the end of the survey, allowing participants to provide feedback on the positive and negative aspects of the application. While this did not fit with the quantitative research approach, this feedback served as a basis for future work on the system.
	%This resulted in the creation of the survey seen in Appendix B. 
	}
	
	\paragraph{}{
	The usability study was conducted on domain experts, such as mechanics, who had previous experience with diagnostic scan tools. Potential candidates were approached and requested to take part in the survey and a time and date for the study was set with the candidates who accepted. On the day of the study, the candidates were given a tablet and an ELM327 device connected to the vehicle. The participants were requested to use the application for up to fifteen minutes and to privately fill out the survey.
	%Potential candidates were approached
	%[HOW SURVEY WAS CARRIED OUT]
	}
	
	\paragraph{}{
	Once all participants had completed the survey, the results were analysed. For the quantitative questions, each answer was transformed into a number between zero and four if the question was positive, such as "I found the look and feel of the application attractive". If the questions was negative, such as "I found the application slow or unresponsive", the answer was transformed into a number between zero and negative four. The average value for each question was calculated to provide an insight into the overall opinion of the usability of the application. The results of this process can be seen in Table \ref{tab:UsabilityScores}.
	}
	
	\paragraph{}{
	The analysis of the results highlighted a few usability issues. For example, during the evaluation it was noticed that the participants frequently mistook the information panels on the home page as buttons that could be used to navigate to the modules and that icons and buttons were too small on the tablet that was used for testing. Despite these issues, given the quantitative analysis of the usability study, it was deemed that the application was usable.
	%[REFLECTIONS]
	}
	
	\begin{table}[ht]
		\begin{center}				
			\begin{tabularx}{\textwidth}{| X | l |}
				\hline
				\textbf{Question} & \textbf{Score}\\
				\hline
				I found the application slow or unresponsive & 0\\
				\hline
				I found it easy to understand the available options on each screen & 3.6\\
				\hline
				I always knew the status of the application & 2.6\\
				\hline
				I felt in control of the application when I was using it & 3.6\\
				\hline
				I found the look and feel was consistent across the application & 3.5\\
				\hline
				I found it easy to recover from any mistakes I made & 3.3\\
				\hline
				I had to go back to the help guide often & -2.3\\
				\hline
				I liked the flow and organisation of the menus / options & 3.3\\
				\hline
				I found it easy to learn how to use the application & 3.6\\
				\hline
				I could understand the information given by the application & 3.6\\
				\hline
				I found the information was displayed in a logical way & 4\\
				\hline
				I found the look and feel of the application attractive & 3.6\\
				\hline
				I found the error messages easy to understand & 3.6\\
				\hline
				I found the instructions and help information useful & 4\\
				\hline
				I found the user input options (buttons, scrolling etc.) did what I expected them to do & 3.6\\
				\hline
			\end{tabularx}
			\caption{Quantitative Results of Usability Study}
			\label{tab:UsabilityScores}
		\end{center}
	\end{table}
	
	%[RESULTS] Information panels on home page were confused for buttons
	
			
	\label{sec:Usability}
}

\section{Software Quality}{
	\label{sec:Quality}
	\paragraph{}{
	Code metrics were gathered as a means of determining the overall software quality. While the code metrics that were gathered are only a snapshot of the software quality at a single point in time, in this case at the end of the development process, if the code metrics are gathered periodically throughout the development process, they can track the increase or decrease in quality over time.
	}
	\paragraph{}{
	The main tool used to capture the code metrics was Visual Studio. The metrics that are available are lines of code, maintainability index, class coupling, cyclomatic complexity and depth of inheritance. The metrics were run on each project: the Android application, the UWP application and the the core library. The results for each can be found in Tables \ref{tab:AndroidApp}, \ref{tab:UWPApp} and \ref{tab:CoreLib} respectively.
	% Tools used to capture metrics
	% Currently give a snaphot of software quality, but by capturing metrics over time we see quality throughout a project	
	}
	
	\subsection*{Lines of Code}
		\paragraph{}{
		This metric indicates the number of lines of executable code. High numbers may indicate that "a type or method is trying to do too much work and should be split up" \cite{CodeMetrics}.
		}
		\paragraph{}{
		The number of lines of code cannot be used to evaluate the quality of the software alone. However,  by comparing the lines of code between each project, the portability of the system can be evaluated. The significant difference between the lines of code in the core library and the platform specific projects highlight that the majority of the system functionality could be shared between platforms without requiring duplication for each platform.
		%Interpretation
		}
		
	\subsection*{Maintainability Index}
		\paragraph{}{
		Maintainability Index is the measure of the maintainability of the code. It is a numerical value between zero and one hundred, which is then used to determine a rating. Values between zero and nine are given a red rating, indicating low maintainability. Values between ten and nineteen are given a yellow rating, indicating moderate maintainability. Finally, values between twenty and one hundred are given a green rating, indicating high maintainability.
		}
		\paragraph{}{
		As all classes were given a green rating, it can be said that the overall maintainability of the system is high. The classes with the lowest maintainability indexes were the factory classes, such as PidFactory and DataConverter. This is because a lot of the logic is stored within the class. A solution would be to create a database or local file to store this information and retrieve it on request instead of storing it in the class itself.
		%Interpretation
		}
	
	\subsection*{Class Coupling}
		\paragraph{}{
		Class coupling is the measure of "coupling to unique classes through parameters, local variables, return types, method calls, generic or template instantiations, base classes, interface implementations, fields defined on external types, and attribute decoration" \cite{CodeMetrics}. Good software design states that classes should have low coupling. High coupling can indicate "a design that is difficult to reuse and maintain because of its many interdependencies on other types." \cite{CodeMetrics}.
		}
		\paragraph{}{
		The class coupling values were higher in the UWP and Android projects when compared to the core library. This can be attributed to the fact that the UWP and Android projects use the classes that are defined in the core library. In the core library, class coupling is lower as it uses primitive and built-in types such as 'string' and 'int' more frequently than user defined classes.
		%Interpretation
		}
	
	\subsection*{Cyclomatic Complexity}
		\paragraph{}{
		Cyclomatic complexity is the measure of "the structural complexity of the code" \cite{CodeMetrics}. Code that has a complex control, or a higher cyclomatic complexity value, may require more tests to achieve good code coverage, making it less maintainable.
		}
		\paragraph{}{
		Overall, the cyclomatic complexity values in the core library was significantly greater than those of the UWP and Android projects. This can be attributed to the fact that all of the business logic is found in the core library. For this reason, there are more conditions to be tested, for example in the DataConverter class, which checks the pid name and the current value, leading to an insurmountable number of paths. Much like the maintainability index, a solution to this would be to create a database or local file and send a single query, rather than using conditional statements.
		%Interpretation
		}
	
	\subsection*{Depth of Inheritance}{
		\paragraph{}{
		Depth of Inheritance represents "the number of class definitions that extend to the root of the class hierarchy" \cite{CodeMetrics}. A higher value represents a deeper hierarchy meaning the code is more difficult to understand where a method or field is defined.
		}
		\paragraph{}{
		As expected, the depth of inheritance values were much greater in the UWP and Android code when compared to the core code. This is because these projects utilise and extend exist classes within their respective frameworks for user interface elements.
		%Interpretation
		}
		

\section{Conclusion}
	\paragraph{}{
	The evaluation of the system, through testing and gathering code metrics for software quality, highlighted that both the functional and non-functional requirements had been satisfied. Furthermore, the usability of the system was evaluated through live field studies with domain experts and the results found that the system had a high level of usability.
	}		
	\paragraph{}{
	}
		
		\begin{table}[ht]
		\begin{scriptsize}			
			\begin{center}				
				\begin{tabularx}{\textwidth}{| l | X | l | l | l | l | l |}
								
				\hline
				\textbf{Package} & \textbf{Class} & \textbf{Lines} & \textbf{MI} & \textbf{CC} & \textbf{Cyc} & \textbf{DoI}\\
				\hline
				Root & AppRoot & 14 & 84 & 9 & 8 & 1\\
				\hline
				Connection & BluetoothDataConnection & 63 & 77 & 24 & 31 & 1\\
				\hline
				\multirow{2}{*}{UI.Codes} & CodesFragment & 32 & 69 & 25 & 16 & 3\\\cline{2-7}
										  & CodeView & 14 & 67 & 7 & 3 & 4\\\cline{2-7}
				\hline
				\multirow{2}{*}{UI.Connection} & ConnectionFragment & 49 & 65 & 32 & 20 & 3\\\cline{2-7}
										  	   & DeviceView & 18 & 78 & 8 & 6 & 5\\\cline{2-7}
				\hline
				\multirow{2}{*}{UI.Data} & DataFragment & 21 & 76 & 25 & 10 & 3\\\cline{2-7}
     							  	     & DataListView & 28 & 63 & 13 & 4 & 5\\\cline{2-7}
				\hline
				\multirow{4}{*}{UI.Host} & HostActivity & 35 & 69 & 36 & 19 & 9\\\cline{2-7}
										 & HostModuleListToggle & 13 & 78 & 5 & 7 & 3\\\cline{2-7}
										 & ModuleListViewAdapter & 17 & 76 & 17 & 7 & 3\\\cline{2-7}
										 & HelpItemsListViewAdapter & 11 & 81 & 11 & 5 & 3\\\cline{2-7}
				\hline
				\end{tabularx}
				\caption{Android Application Metrics}
				\label{tab:AndroidApp}
			\end{center}
		\end{scriptsize}
		\end{table}		
		
		\begin{table}[ht]
		\begin{scriptsize}			
			\begin{center}				
				\begin{tabularx}{\textwidth}{| l | X | l | l | l | l | l |}
								
				\hline
				\textbf{Package} & \textbf{Class} & \textbf{Lines} & \textbf{MI} & \textbf{CC} & \textbf{Cyc} & \textbf{DoI}\\
				\hline				
				Root & App & 36 & 70 & 40 & 11 & 2\\
				\hline
				Connection & BluetoothDataConnection & 64 & 77 & 25 & 24 & 1\\
				\hline
				\multirow{9}{*}{UI} & HostView & 74 & 57 & 55 & 28 & 7\\\cline{2-7}
									& HomePage & 8 & 64 & 17 & 3 & 7\\\cline{2-7}
									& ModuleHintView & 9 & 63 & 6 & 6 & 6\\\cline{2-7}
									& ConnectionPage & 37 & 61 & 42 & 19 & 7\\\cline{2-7}
									& DTCPage & 16 & 70 & 27 & 7 & 7\\\cline{2-7}
									& DTCView & 8 & 77 & 6 & 2 & 6\\\cline{2-7}
									& DataPage & 13 & 72 & 20 & 4 & 7\\\cline{2-7}
									& DataView & 17 & 75 & 18 & 10 & 6\\\cline{2-7}
									& DataGraph & 61 & 57 & 38 & 21 & 6\\\cline{2-7}
				\hline
				\end{tabularx}
				\caption{UWP Application Metrics}
				\label{tab:UWPApp}
			\end{center}
		\end{scriptsize}
		\end{table}		
		
		\begin{table}[ht]
		\begin{scriptsize}			
			\begin{center}				
				\begin{tabularx}{\textwidth}{| l | X | l | l | l | l | l |}
								
				\hline
				\textbf{Package} & \textbf{Class} & \textbf{Lines} & \textbf{MI} & \textbf{CC} & \textbf{Cyc} & \textbf{DoI}\\
				\hline
				\multirow{5}{*}{Connection} & IDevice & 0 & 100 & 0 & 3 & 0\\\cline{2-7}
											& BluetoothConnectionDevice & 10 & 92 & 1 & 7 & 1\\\cline{2-7}
											& IDataConnection & 0 & 100 & 6 & 12 & 0\\\cline{2-7}					
											& SimulationDataConnection & 39 & 82 & 18 & 22 & 1\\\cline{2-7}
											& ConnectionManager & 6 & 93 & 2 & 5 & 1\\\cline{2-7}
				\hline
				\multirow{4}{*}{Host} & IHost & 0 & 100 & 3 & 3 & 0\\\cline{2-7}
									  & Host & 30 & 76 & 13 & 20 & 1\\\cline{2-7}
									  & IHostViewModel & 0 & 100 & 4 & 5 & 0\\\cline{2-7}
									  & HostViewModel & 43 & 75 & 26 & 26 & 1\\\cline{2-7}
				\hline
				\multirow{7}{*}{Modules.Base} & IModule & 0  & 100 & 4 & 4 & 0\\\cline{2-7}
									  		  & IModuleViewModel & 0  & 100 & 5 & 6 & 0\\\cline{2-7}
									  		  & ICommunicationSystem & 0  & 100 & 0 & 3 & 0\\\cline{2-7}
									  		  & IHelpItem & 0 & 100 & 0 & 2 & 0\\\cline{2-7}
									  		  & HelpItem & 7  & 93 & 1 & 5 & 1\\\cline{2-7}
									  		  & HelpItemFactory & 23 & 76 & 6 & 10 & 1\\\cline{2-7}
									  		  & RelayCommand & 20 & 83 & 7 & 11 & 1\\\cline{2-7}
				\hline
				\multirow{11}{*}{Modules.Codes} & ICode & 0 & 100 & 0 & 6 & 0\\\cline{2-7}
											    & Code & 18 & 91 & 1 & 10 & 1\\\cline{2-7}
											    & CodeFactory & 71 & 32 & 2 & 159 & 1\\\cline{2-7}
											    & ICodeViewModel & 0 & 100 & 1 & 5 & 0\\\cline{2-7}
											    & CodeViewModel & 16 & 91 & 2 & 11 & 1\\\cline{2-7}
											    & IDtcCommsSystem & 0 & 100 & 4 & 4 & 0\\\cline{2-7}
											    & DTCCommunicationSystem & 18  & 82 & 13 & 8 & 1\\\cline{2-7}
											    & IDtcModule & 0 & 100 & 5 & 7 & 0\\\cline{2-7}
											    & DTCModule & 52 & 81 & 23 & 23 & 1\\\cline{2-7}
											    & IDtcModuleViewModel & 0  & 100 & 5 & 4 & 0\\\cline{2-7}
											    & DTCModuleViewModel & 75 & 75 & 29 & 46 & 1\\\cline{2-7}
				\hline
				\multirow{4}{*}{Modules.Connection} & IConnectionModule & 0 & 100 & 5 & 4 & 0\\\cline{2-7}
									  		  		& BluetoothModule & 49 & 81 & 20 & 26 & 1\\\cline{2-7}
									  		  		& IConnectionModuleViewModel & 0 & 100 & 5 & 7 & 0\\\cline{2-7}
									  		  		& BluetoothModuleViewModel & 54 & 82 & 21 & 30 & 1\\\cline{2-7}
				\hline
				\multirow{17}{*}{Modules.Data} & IPid & 0 & 100 & 3 & 5 & 0\\\cline{2-7}
											   & Pid & 28 & 90 & 10 & 18 & 1\\\cline{2-7}
											   & PidFactory & 61 & 35 & 2 & 131 & 1\\\cline{2-7}
											   & IDataItem & 0 & 100 & 1 & 4 & 0\\\cline{2-7}
											   & DataItem & 12 & 92 & 2 & 8 & 1\\\cline{2-7}
											   & DataConverter & 113 & 35 & 5 & 158 & 1\\\cline{2-7}
											   & IDataCommsSystem & 0 & 100 & 4 & 3 & 0\\\cline{2-7}
											   & DataCommunicationSystem & 17 & 81 & 14 & 7 & 1\\\cline{2-7}
											   & IDataModule & 0 & 100 & 5 & 6 & 0\\\cline{2-7}
											   & DataModule & 49 & 81 & 25 & 21 & 1\\\cline{2-7}
											   & IDataModuleViewModel & 0 & 100 & 5 & 2 & 0\\\cline{2-7}
											   & DataModuleViewModel & 92 & 80 & 33 & 62 & 1\\\cline{2-7}
											   & IDataViewModel & 0 & 100 & 4 & 9 & 0\\\cline{2-7}
											   & IDataListViewModel & 0 & 100 & 2 & 1 & 0\\\cline{2-7}
											   & DataListViewModel & 42 & 82 & 12 & 29 & 1\\\cline{2-7}
											   & IDataGraphViewModel & 0 & 100 & 2 & 3 & 0\\\cline{2-7}
											   & DataGraphViewModel & 48 & 82 & 11 & 31 & 1\\\cline{2-7}
				\hline
				\multirow{4}{*}{Modules.Home} & IHomeModule & 0 & 100 & 2 & 0 & 0\\\cline{2-7}
									  		  & HomeModule & 21 & 83 & 14 & 11 & 1\\\cline{2-7}
									  		  & IHomeModuleViewModel & 0 & 100 & 2 & 0 & 0\\\cline{2-7}
									  		  & HomeModuleViewModel & 23 & 83 & 14 & 13 & 1\\\cline{2-7}				
				\hline
				
				\end{tabularx}
				\caption{Core Library Metrics}
				\label{tab:CoreLib}
			\end{center}
		\end{scriptsize}
		\end{table}

	}
}

	
	
	%\paragraph{}{
	%Testing of the application is essential in ensuring that the system satisfies the functional and non-functional requirements.
	%}
	%\paragraph{}{
	%Ensuring the application displays the correct data is paramount, as an erroneous data value could cause confusion for the end user. Functional testing will ensure that data is not incorrectly converted by the communication system or incorrectly displayed by the UI. The output of the application will be compared with the output of the similar applications outlined in section 2.4, to ensure there is no disparity. 
	%}
	%\paragraph{}{
	%Unit tests will be written for each component of the application. These unit tests will be run before a change is ready to be committed to the source control repository. If any unit tests fail, that have not failed prior to the implementation of the recent changes, the code will not be committed until the underlying causes have been fixed. This helps to minimise the number of new defects introduced as part of implementing new features.
	%}